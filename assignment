from google.colab import drive
drive.mount("/content/drive")

import spacy
from collections import Counter

def remove_punctuation_custom(text):
    amharic_punctuation = "፡።፣፤፥፦፧፡።፣፤፥፦?"
    cleaned_text = ''.join(char for char in text if char not in amharic_punctuation)
    return cleaned_text
  
def extract_ngrams(corpus, n):
    ngrams = []
    doc=corpus.split()
    ngrams =[" ".join(doc[i:i + n]) for i in range(len(doc) - n + 1)]
    return ngrams

unigram = []
bigram = []
trigram = []
fourgram = []

with open("/content/drive/MyDrive/Colab Notebooks/GPAC.txt", 'r', encoding="utf-8", errors="ignore") as file:
    chunk=file.read(5000)      
    data = remove_punctuation_custom(chunk)
    unigram.extend(extract_ngrams(data, 1))
    bigram.extend(extract_ngrams(data, 2))
    trigram.extend(extract_ngrams(data, 3))
    fourgram.extend(extract_ngrams(data, 4))

uni_counter = Counter(unigram)
bigram_counter = Counter(bigram)
trigram_counter = Counter(trigram)
fourgram_counter = Counter(fourgram)

print(unigram[:10])
print()
print(bigram[:10])
print()
print(trigram[:10])



####Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n

def ngram_probability(ngram_counter):
    total_length = sum(ngram_counter.values())
    ngram_probability = {token: count / total_length for token, count in ngram_counter.items()}
    sorted_ngrams = sorted(ngram_probability.items(), key=lambda x: x[1], reverse=True)
    return sorted_ngrams[:10]

bigram_probability=ngram_probability(bigram_counter)
unigram_probability=ngram_probability(uni_counter)

print(unigram_probability)
print()
print(bigram_probability)


##### What is the probability of the sentence. "ኢትዮጵያ ታሪካዊ ሀገር ናት "

def calculate_sentence_probability(sentence, ngram_counter, n):
    words = remove_punctuation_custom(sentence).split()
    ngrams = extract_ngrams(words, n)

    sentence_probability = 1.0
    for i in range(len(ngrams) - 1):  
        current_ngram = " ".join(ngrams[i:i + n])

        if current_ngram in ngram_counter:
            conditional_probability = ngram_counter[current_ngram] / sum(ngram_counter.values())
        else:
            conditional_probability = 0.0

        sentence_probability *= conditional_probability

    return sentence_probability

example_sentence = "ኢትዮጵያ ታሪካዊ ሀገር ናት"

bigram_probability = calculate_sentence_probability(example_sentence, bigram_counter, 2)
trigram_probability = calculate_sentence_probability(example_sentence, trigram_counter, 3)

print(f"Bigram Probability of the sentence '{example_sentence}': {bigram_probability}")
print(f"Trigram Probability of the sentence '{example_sentence}': {trigram_probability}")


######Generate random sentences using n-grams

######As n increases, sentences tend to be more contextualy appropriate and meaning full , but they might also become more repetitive


import random

def generate_random_sentence(ngram_counter, n):
    sentence = []

    for i in range(5):
        if n == 1:
            next_word = random.choice(list(ngram_counter.keys()))
        else:
            last_ngram = " ".join(sentence[-(n-1):])
            possible_next_words = [word.split()[-1] for word in ngram_counter.keys() if word.startswith(last_ngram)]
            next_word = random.choice(possible_next_words) if possible_next_words else random.choice(list(ngram_counter.keys()))

        sentence.append(next_word)

    return " ".join(sentence)

random_unigram_sentence = generate_random_sentence(uni_counter, 1)
random_bigram_sentence = generate_random_sentence(bigram_counter, 2)
random_trigram_sentence = generate_random_sentence(trigram_counter, 3)
random_fourgram_sentence = generate_random_sentence(fourgram_counter, 4)


print(f"Bigram Sentence: {random_bigram_sentence}")
print(f"Trigram Sentence: {random_trigram_sentence}")




#####Evaluate these Language Models Using Intrinsic Evaluation Method

#######for the intrinsic evaluation i am calculating perplexity for each ngrams


import math

def calculate_perplexity(test_set, ngram_counters, k=1):
    N = sum(len(sentence.split()) for sentence in test_set)
    perplexities = []

    for n, counter in enumerate(ngram_counters, start=1):
        total_log_likelihood = 0

        for sentence in test_set:
            ngrams = extract_ngrams(sentence, n)
            for ngram in ngrams:
                probability = (counter[ngram] + k) / (sum(counter.values()) + len(counter))
                total_log_likelihood += math.log(probability)

        perplexity = math.exp(-(total_log_likelihood / N))
        perplexities.append(perplexity)
        print(f"Perplexity for n={n}: {perplexity}")

    return perplexities

test_set = ["ኢትዮጵያ ታሪካዊ ሀገር ናት", "ሰላም እንደ ገና ሆኖ አነስተኛ እንደ ሆነ ነው", "በሺህ አመታት በተከታታይ ስታዘጋጅ የኖረች","የአለም የእግር ኳስ" ]
perplexities = calculate_perplexity(test_set, [uni_counter, bigram_counter, trigram_counter, fourgram_counter], k=1)



######Evaluate these Language Models Using Extrinsic Evaluation Method

#######by writing function to predict the next word and seeing how it performs

def predict_next_word(word, bigram_frequency):
    next_word_options = [next_word for (current_word, next_word), freq in bigram_frequency.items() if current_word == word]
    return random.choice(next_word_options) if next_word_options else None

sentence = ['ኢትዮጵያ']
for _ in range(5): 
    current_word = sentence[-1]
    next_word = predict_next_word(current_word, bigram_counter)
    if next_word:
        sentence.append(next_word)
    else:
        break

generated_sentence = " ".join(sentence)
generated_sentence
