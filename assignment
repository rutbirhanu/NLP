from google.colab import drive
drive.mount("/content/drive")

import spacy
from collections import Counter

def remove_punctuation_custom(text):
    amharic_punctuation = "፡።፣፤፥፦፧፡።፣፤፥፦?"
    cleaned_text = ''.join(char for char in text if char not in amharic_punctuation)
    return cleaned_text
  
def extract_ngrams(corpus, n):
    ngrams = []
    doc=corpus.split()
    ngrams =[" ".join(doc[i:i + n]) for i in range(len(doc) - n + 1)]
    return ngrams

unigram = []
bigram = []
trigram = []
fourgram = []

with open("/content/drive/MyDrive/Colab Notebooks/GPAC.txt", 'r', encoding="utf-8", errors="ignore") as file:
    chunk=file.read(5000)      
    data = remove_punctuation_custom(chunk)
    unigram.extend(extract_ngrams(data, 1))
    bigram.extend(extract_ngrams(data, 2))
    trigram.extend(extract_ngrams(data, 3))
    fourgram.extend(extract_ngrams(data, 4))

uni_counter = Counter(unigram)
bigram_counter = Counter(bigram)
trigram_counter = Counter(trigram)
fourgram_counter = Counter(fourgram)

print(unigram[:10])
print()
print(bigram[:10])
print()
print(trigram[:10])



####Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n

def ngram_probability(ngram_counter):
    total_length = sum(ngram_counter.values())
    ngram_probability = {token: count / total_length for token, count in ngram_counter.items()}
    sorted_ngrams = sorted(ngram_probability.items(), key=lambda x: x[1], reverse=True)
    return sorted_ngrams[:10]

bigram_probability=ngram_probability(bigram_counter)
unigram_probability=ngram_probability(uni_counter)

print(unigram_probability)
print()
print(bigram_probability)


##### What is the probability of the sentence. "ኢትዮጵያ ታሪካዊ ሀገር ናት "

def calculate_sentence_probability(sentence, ngram_counter, n):
    words = remove_punctuation_custom(sentence).split()
    ngrams = extract_ngrams(words, n)

    sentence_probability = 1.0
    for i in range(len(ngrams) - 1):  
        current_ngram = " ".join(ngrams[i:i + n])

        if current_ngram in ngram_counter:
            conditional_probability = ngram_counter[current_ngram] / sum(ngram_counter.values())
        else:
            conditional_probability = 0.0

        sentence_probability *= conditional_probability

    return sentence_probability

example_sentence = "ኢትዮጵያ ታሪካዊ ሀገር ናት"

bigram_probability = calculate_sentence_probability(example_sentence, bigram_counter, 2)
trigram_probability = calculate_sentence_probability(example_sentence, trigram_counter, 3)

print(f"Bigram Probability of the sentence '{example_sentence}': {bigram_probability}")
print(f"Trigram Probability of the sentence '{example_sentence}': {trigram_probability}")


######Generate random sentences using n-grams

######As n increases, sentences tend to be more contextualy appropriate and meaning full , but they might also become more repetitive


import random

def generate_random_sentence(ngram_counter, length=5):
    sentence = []
    first_ngram = random.choice(list(ngram_counter.keys()))

    for _ in range(length):
        sentence.extend(first_ngram)
        next_ngram_options = [gram for gram in ngram_counter if gram[0] == first_ngram[-1]]
        if next_ngram_options:
            first_ngram = random.choice(next_ngram_options)
        else:
            break


    final_sentence = " ".join([word for bigram in sentence for word in bigram])

    return final_sentence

rand_sentence = generate_random_sentence(bigram_counter)
print(rand_sentence)





#####Evaluate these Language Models Using Intrinsic Evaluation Method

#######for the intrinsic evaluation i am calculating perplexity for each ngrams

vocabulary_size = len(set(unigram))

def calculate_perplexity(ngram_counter, ngram, vocabulary_size):
    total_token = len(ngram) 

    smoothed_probability = {token: (count + 1) / (ngram_counter[token] + vocabulary_size) for token, count in ngram_counter.items()}

    total_probability = 1
    for token in ngram:
        total_probability *= smoothed_probability[token]

    if total_token > 0:
        perplexity = (1 / total_probability) ** (1 / total_token)
        return perplexity
    else:
        return None





######Evaluate these Language Models Using Extrinsic Evaluation Method

#######by writing function to predict the next word and seeing how it performs

def predict_next_word(word, bigram_frequency):
    next_word_options = [next_word for (current_word, next_word), freq in bigram_frequency.items() if current_word == word]
    return random.choice(next_word_options) if next_word_options else None

sentence = ['ኢትዮጵያ']
for _ in range(5): 
    current_word = sentence[-1]
    next_word = predict_next_word(current_word, bigram_counter)
    if next_word:
        sentence.append(next_word)
    else:
        break

generated_sentence = " ".join(sentence)
generated_sentence
