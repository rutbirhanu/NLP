from google.colab import drive
drive.mount("/content/drive")

import spacy
from collections import Counter

nlp = spacy.load("en_core_web_sm")
nlp.max_length = 500000000


def extract_ngrams(doc, n):
    ngrams = []
    for i in range(len(doc) - n + 1):
        ngram = " ".join([token.text for token in doc[i:i + n]])
        ngrams.append(ngram)
    return ngrams

unigram = []
bigram = []
trigram = []
fourgram = []

with open("/content/drive/MyDrive/Colab Notebooks/GPAC.txt", 'r', encoding="utf-8", errors="ignore") as file:
    for chunk in iter(lambda: file.read(4096), ''):
        data = nlp(chunk)

        unigram.extend(extract_ngrams(data, 1))
        bigram.extend(extract_ngrams(data, 2))
        trigram.extend(extract_ngrams(data, 3))
        fourgram.extend(extract_ngrams(data, 4))

uni_counter = Counter(unigram)
bigram_counter = Counter(bigram)
trigram_counter = Counter(trigram)
fourgram_counter = Counter(fourgram)



####Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n

def ngram_probability(ngram_counter):
    total_length = sum(ngram_counter.values())
    ngram_probability = {token: count / total_length for token, count in ngram_counter.items()}
    sorted_ngrams = sorted(ngram_probability.items(), key=lambda x: x[1], reverse=True)
    return sorted_ngrams[:10]

bigram_probability=ngram_probability(bigram_counter)




##### What is the probability of the sentence. "ኢትዮጵያ ታሪካዊ ሀገር ናት "

total_length=sum(uni_counter.values())
first_word=uni_counter['ኢትዮጵያ']/total_length 
second_word=uni_counter['ታሪካዊ']/total_length 
third_word=uni_counter['ሀገር']/total_length 
fourth_word=uni_counter['ናት']/total_length 

sentence_probablity=first_word*second_word*third_word*fourth_word
sentence_probablity





######Generate random sentences using n-grams

######As n increases, sentences tend to be more contextualy appropriate and meaning full , but they might also become more repetitive


import random

def generate_random_sentence(ngram_counter, length=5):
    sentence = []
    first_ngram = random.choice(list(ngram_counter.keys()))

    for _ in range(length):
        sentence.extend(first_ngram)
        next_ngram_options = [gram for gram in ngram_counter if gram[0] == first_ngram[-1]]
        if next_ngram_options:
            first_ngram = random.choice(next_ngram_options)
        else:
            break


    final_sentence = " ".join([word for bigram in sentence for word in bigram])

    return final_sentence

rand_sentence = generate_random_sentence(bigram_counter)
print(rand_sentence)




#####Evaluate these Language Models Using Intrinsic Evaluation Method

#######for the intrinsic evaluation i am calculating perplexity for each ngrams

vocabulary_size = len(set(unigram))

def calculate_perplexity(ngram_counter, ngram, vocabulary_size):
    total_token = len(ngram) 

    smoothed_probability = {token: (count + 1) / (ngram_counter[token] + vocabulary_size) for token, count in ngram_counter.items()}

    total_probability = 1
    for token in ngram:
        total_probability *= smoothed_probability[token]

    if total_token > 0:
        perplexity = (1 / total_probability) ** (1 / total_token)
        return perplexity
    else:
        return None
